{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beautiful II.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "UxfXLgtqHKK8",
        "36e6eJ5-zCu4",
        "FU4gG-CcHM2_"
      ],
      "toc_visible": true,
      "mount_file_id": "11Tev6B2oNVxujk6RfoclZBCcSkd_7SUM",
      "authorship_tag": "ABX9TyMYeVlJhhwX5FgRHd+dIkLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakashaditya369/AdditionQuiz/blob/master/Beautiful_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o16KO0DzBNy"
      },
      "source": [
        "# !pip install mtcnn\n",
        "!pip install ray[tune]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNFUHhxj5mCX",
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "!unzip /content/drive/MyDrive/Beauty/dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z76Pu15NrTlk"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPE9Xz5ao2ae",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Functions\n",
        "def progress_batch(i,Len, width=30):\n",
        "  left = int(width * (i+1)*100/Len) // 100\n",
        "  right = width - left\n",
        "  print('\\r[', '*' * left, ' ' * right, ']',f'{str(i+1)}/{str(Len)}',\n",
        "        sep='', end='', flush=True)\n",
        "def progress(percent=0, width=30):\n",
        "  left = int(width * percent) // 100\n",
        "  right = width - left\n",
        "  print('\\r[', '#' * left, ' ' * right, ']',\n",
        "        f' {percent:.2f}%',\n",
        "        sep='', end='', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6dccxAZwMhy"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "# from mtcnn import MTCNN\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# CUDA_LAUNCH_BLOCKING=1\n",
        "# detector = MTCNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqtArkYpyFkh"
      },
      "source": [
        "def preprocess_image(img_add):\n",
        "  img = cv2.cvtColor(cv2.imread(img_add), cv2.COLOR_BGR2RGB)\n",
        "  result = detector.detect_faces(img)\n",
        "  if len(result)>0:\n",
        "    result = result[0]\n",
        "    keypoints = result['keypoints']\n",
        "    leftEyeCenter = keypoints['left_eye']\n",
        "    rightEyeCenter = keypoints['right_eye']\n",
        "    dY = rightEyeCenter[1] - leftEyeCenter[1]\n",
        "    dX = rightEyeCenter[0] - leftEyeCenter[0]\n",
        "    angle = np.degrees(np.arctan2(dY, dX))\n",
        "    desiredFaceWidth = 227\n",
        "    desiredFaceHeight = 227\n",
        "    desiredLeftEye=(0.33, 0.35)\n",
        "    desiredRightEyeX = 1.0 - desiredLeftEye[0]\n",
        "    dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
        "    desiredDist = (desiredRightEyeX - desiredLeftEye[0])\n",
        "    desiredDist *= desiredFaceWidth\n",
        "    scale = desiredDist / dist\n",
        "    eyesCenter = ((leftEyeCenter[0] + rightEyeCenter[0]) // 2, (leftEyeCenter[1] + rightEyeCenter[1]) // 2)\n",
        "    M = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\n",
        "    tX = desiredFaceWidth * 0.5\n",
        "    tY = desiredFaceHeight * desiredLeftEye[1]\n",
        "    M[0, 2] += (tX - eyesCenter[0])\n",
        "    M[1, 2] += (tY - eyesCenter[1])\n",
        "    (w, h) = (desiredFaceWidth, desiredFaceHeight)\n",
        "    (y2,x2,y1,x1) = result['box']\n",
        "    img = cv2.warpAffine(img, M, (w, h),flags=cv2.INTER_CUBIC)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKeJZmm_qcK9"
      },
      "source": [
        "def preprocess():\n",
        "  print(\"Reading Files\")\n",
        "  CSV_FILE = \"/content/dataset/dataset.csv\"\n",
        "  IMG_PATH = \"/content/dataset/images/\"\n",
        "  final_image_size = (227, 227)\n",
        "  df = pd.read_csv(CSV_FILE)\n",
        "  labels = list(df[\"Score\"])\n",
        "  labels = [i-1 for i in labels]\n",
        "  image_add = df[\"Image\"]\n",
        "  images = []\n",
        "  i = 0\n",
        "  print(\"Face Detecting and building the array.\")\n",
        "  for image in image_add:\n",
        "    progress((i+1)/len(image_add)*100)\n",
        "    img = preprocess_image(IMG_PATH+image)\n",
        "    images.append(img)\n",
        "    i+=1\n",
        "  return image_add, images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AwUku4Avp5w"
      },
      "source": [
        "add, images, labels = preprocess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSyTExFr4uyk"
      },
      "source": [
        "images = np.load(\"/content/drive/MyDrive/Beauty/beauty_images.npy\")\n",
        "labels = np.load(\"/content/drive/MyDrive/Beauty/beauty_labels.npy\")\n",
        "# greyed_images = np.dot(images[:,:,:,:3], [0.299, 0.587, 0.114]).reshape(images.shape[0],227,227,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDXLAs4q5tRn"
      },
      "source": [
        "#Data Augmentation part\n",
        "greyed_imaged = np.dot(images[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "print(greyed_imaged.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOcGjpq6Cyiy"
      },
      "source": [
        "temp_grey_images = []\n",
        "for i in range(greyed_imaged.shape[0]):\n",
        "  imgh = greyed_imaged[i]\n",
        "  imgs = np.flip(imgh, 1)\n",
        "  temp_grey_images.append(imgs)\n",
        "temp_grey_images = np.array(temp_grey_images)\n",
        "greyed_images = np.concatenate((greyed_imaged, temp_grey_images)).reshape(2*images.shape[0], 128, 128, 1)\n",
        "labels = np.concatenate((labels, labels))\n",
        "np.save(\"/content/drive/MyDrive/Beauty/grey_augment_images.npy\", greyed_images)\n",
        "np.save(\"/content/drive/MyDrive/Beauty/augment_labels.npy\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q0o3-QjiPlD"
      },
      "source": [
        "greyed_images = np.load(\"/content/drive/MyDrive/Beauty/grey_augment_images.npy\")\n",
        "labels = np.load(\"/content/drive/MyDrive/Beauty/augment_labels.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1Aub4UvejTt"
      },
      "source": [
        "shuffle = np.random.randint(0,greyed_images.shape[0], 20)\n",
        "for i in shuffle:\n",
        "  plt.imshow(greyed_images[i][:,:,0])\n",
        "  plt.show()\n",
        "  print(labels[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjA1tygXbfj_"
      },
      "source": [
        "shit = []\n",
        "label = []\n",
        "for i in range(len(images)):\n",
        "  if images[i] is not None:\n",
        "    shit.append(cv2.resize(images[i], (227,227), interpolation = cv2.INTER_AREA))\n",
        "    label.append(labels[i])\n",
        "labels = np.array(label)\n",
        "images = np.array(shit)\n",
        "np.save(\"/content/drive/MyDrive/Beauty/beauty_images\", images)\n",
        "np.save(\"/content/drive/MyDrive/Beauty/beauty_labels\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC_jWGNAfWSu"
      },
      "source": [
        "print(len(images),len(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrYzuMz3zxS1"
      },
      "source": [
        "class ImageLabel(Dataset):\n",
        "  def __init__(self,image_tensor,label_tensor):\n",
        "    self.image = image_tensor\n",
        "    self.labels = label_tensor\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "  def __getitem__(self, idx):\n",
        "    return (self.image[idx],self.labels[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7i8TnnNxwK5"
      },
      "source": [
        "def get_dataset(images_cropped, labels, size = (256, 256)):\n",
        "  #Distributing Data And Converting to Torch Tensor\n",
        "  shuffle = np.random.permutation(len(labels))\n",
        "  '''\n",
        "  images = []\n",
        "  labels = [i-1 for i in labels]\n",
        "  for img in images_cropped:\n",
        "    images.append(cv2.resize(img, size, interpolation = cv2.INTER_AREA))\n",
        "  '''\n",
        "  ratio = int(4/5*len(labels))\n",
        "  train_num = shuffle[:ratio]\n",
        "  val_num = shuffle[ratio:]\n",
        "  print(\"Total Data: \", len(labels))\n",
        "  '''\n",
        "  train_label = torch.tensor(np.array(labels)[train_num])\n",
        "  train_image = torch.tensor(np.array(images)[train_num].transpose(0,3,1,2), dtype=torch.float)\n",
        "  val_label = torch.tensor(np.array(labels)[val_num])\n",
        "  val_image = torch.tensor(np.array(images)[val_num].transpose(0,3,1,2), dtype = torch.float)\n",
        "  '''\n",
        "  train_label = torch.tensor(labels[train_num])\n",
        "  train_image = torch.tensor(images_cropped[train_num].transpose(0,3,1,2), dtype=torch.float)\n",
        "  val_label = torch.tensor(labels[val_num])\n",
        "  val_image = torch.tensor(images_cropped[val_num].transpose(0,3,1,2), dtype = torch.float)\n",
        "  # print(\"Training Data: \",train_label.shape, train_image.shape)\n",
        "  # print(\"Validate Data: \", val_label.shape, val_image.shape)\n",
        "  train_data = ImageLabel(train_image, train_label)\n",
        "  val_data = ImageLabel(val_image, val_label)\n",
        "  return train_data, val_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxfXLgtqHKK8"
      },
      "source": [
        "# Model Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-TUGG6KxozQ"
      },
      "source": [
        "## AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKyLG8ZU1huo"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, l1 = 4096, l2 = 200):\n",
        "        '''Input Image Size: (227, 227)'''\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 96, kernel_size = 11)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size = 5)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size = 3)\n",
        "        self.conv4 = nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = 3)\n",
        "        self.conv5 = nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = 3)\n",
        "        self.fc1 = nn.Linear(4096, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 5)\n",
        "    def forward(self, x, verbose=False):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.conv5(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-GAyc2PxrdL"
      },
      "source": [
        "## Some Other Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh-FLZaXxuwq"
      },
      "source": [
        "class Model_FER(nn.Module):\n",
        "  def __init__(self, l1= 128, l2 = 32):\n",
        "    '''Input Image Size: (227, 227)'''\n",
        "    super(Model_FER, self).__init__()\n",
        "    self.conv0 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = 11)\n",
        "    self.conv1 = nn.Conv2d(in_channels = 64, out_channels = 96, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 5)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 224, out_channels = 128, kernel_size = 3)\n",
        "    self.conv4 = nn.Conv2d(128, 64, 3)\n",
        "    self.batchnorm = nn.BatchNorm2d(64)\n",
        "    self.dropout1 = nn.Dropout(0.4)\n",
        "    self.dropout2 = nn.Dropout(0.4)\n",
        "    self.fc1 = nn.Linear(64, l1)\n",
        "    self.fc2 = nn.Linear(l1, l2)\n",
        "    self.fc3 = nn.Linear(l2, 5)\n",
        "  def forward(self, x, verbose = False):\n",
        "    x = self.conv0(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=3)\n",
        "    x =  self.batchnorm(x)\n",
        "    x1 = self.conv1(x)\n",
        "    x1 = F.relu(x1)\n",
        "    x1 = F.max_pool2d(x1, kernel_size = 3)\n",
        "    x2 = self.conv2(x)\n",
        "    x2 = F.relu(x2)\n",
        "    x2 = F.max_pool2d(x2, kernel_size = 3)\n",
        "    x3 = torch.cat((x1, x2), dim = 1)\n",
        "    x3 = self.dropout1(x3)\n",
        "    x3 = self.conv3(x3)\n",
        "    x3 = F.relu(x3)\n",
        "    x3 = F.max_pool2d(x3, kernel_size = 3)\n",
        "    x4  = self.conv4(x3)\n",
        "    x4 = F.relu(x4)\n",
        "    x = torch.flatten(x4, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36e6eJ5-zCu4"
      },
      "source": [
        "## Test Your Model Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0zY-44DzBAS"
      },
      "source": [
        "alex_train_dataset, alex_val_dataset = get_dataset(greyed_images,labels , (227, 227))\n",
        "train_dl = DataLoader(alex_train_dataset,batch_size = 5,shuffle = True)\n",
        "for i in train_dl:\n",
        "  image, label = i\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvpM59kM1e4i"
      },
      "source": [
        "model = Model_FER()\n",
        "model.to(device)\n",
        "label = label.to(device)\n",
        "model(image.to(device))\n",
        "# output = model(image.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU4gG-CcHM2_"
      },
      "source": [
        "# Training And Validation and Testing on real data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mB6rtCd5HQ0"
      },
      "source": [
        "def train(model, dataset, epochs, criterion, optimizer, batch_size = 50):\n",
        "  train_dl = DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
        "  length = len(train_dl)\n",
        "  data_length = dataset.__len__()\n",
        "  history = {}\n",
        "  history['train_acc'] = []\n",
        "  history['train_loss'] = []\n",
        "  history['val_acc'] = []\n",
        "  history['val_loss'] = []\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    print(\"Epoch: {}\".format(epoch+1))\n",
        "    for i, data in enumerate(train_dl):\n",
        "      progress_batch(i,length)\n",
        "      image,label = data[0].to(device),data[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(image)\n",
        "      loss = criterion(outputs, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss+=loss.item()\n",
        "    running_loss/=data_length\n",
        "    print(\"   Loss:\",f' {running_loss:.5f}')\n",
        "    train_loss, train_acc = evaluate(Alexmodel, alex_train_dataset, Alexcriterion)\n",
        "    val_loss, val_acc = evaluate(Alexmodel,alex_val_dataset, Alexcriterion)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    print(f'\\n train_loss: {train_loss:.4f} :: train_acc: {train_acc:.2f} :: val_loss: {val_loss:.4f} :: val_acc: {val_acc:.2f}')\n",
        "  print(\"Finished Training\")\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i09qoDrsD2Ae"
      },
      "source": [
        "def evaluate(model,dataset,criterion, batch_size = 50):\n",
        "  correct = 0\n",
        "  total = dataset.__len__()\n",
        "  total_loss = 0\n",
        "  val_dl = DataLoader(dataset,batch_size = batch_size)\n",
        "  with torch.no_grad():\n",
        "    for data in val_dl:\n",
        "      image,label = data[0].to(device),data[1].to(device)\n",
        "      outputs = model(image)\n",
        "      loss = criterion(outputs,label)\n",
        "      outputs = outputs.detach()\n",
        "      outputs = torch.argmax(outputs, dim = 1)\n",
        "      result = torch.sum(outputs==label)\n",
        "      correct+=result.item()\n",
        "      total_loss+=loss.item()\n",
        "  total_loss/=total\n",
        "  accuracy = correct/total\n",
        "  return total_loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77m0IGt-xGn9"
      },
      "source": [
        "```\n",
        "Alexmodel = Model_FER()\n",
        "Alexmodel.to(device)\n",
        "Alexcriterion = nn.NLLLoss()\n",
        "Alexoptimizer = optim.Adadelta(Alexmodel.parameters(), lr=1e-1, weight_decay=1e-5)\n",
        "```\n",
        "\n",
        "**gives** (30 iterations only)\n",
        "\n",
        "```\n",
        "Train Accuracy: \n",
        "Accuracy: 0.8811363636363636 || Loss: 0.00647\n",
        "Accuracy: 0.7254545454545455 || Loss: 0.01646\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_fp6ECxOKBT"
      },
      "source": [
        "Alexmodel = Model_FER()\n",
        "Alexmodel.to(device)\n",
        "Alexcriterion = nn.NLLLoss()\n",
        "Alexoptimizer = optim.Adadelta(Alexmodel.parameters(), lr=1e-1, weight_decay=1)\n",
        "alex_train_dataset, alex_val_dataset = get_dataset(greyed_images, labels, (227, 227))\n",
        "history = train(Alexmodel, alex_train_dataset,20, Alexcriterion, Alexoptimizer,30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsOFCOJIdNV8"
      },
      "source": [
        "plt.title(\"Accuracies:\")\n",
        "plt.plot(history['train_acc'], label = 'train_acc')\n",
        "plt.plot(history['val_acc'], label ='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.title(\"Losses:\")\n",
        "plt.plot(history['train_loss'], label = 'train_loss')\n",
        "plt.plot(history['val_loss'], label = 'val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwbNywQICtHY"
      },
      "source": [
        "print(\"Train Accuracy: \")\n",
        "evaluate(Alexmodel, alex_train_dataset, Alexcriterion)\n",
        "print(\"Validation Accuracy:\")\n",
        "evaluate(Alexmodel, alex_val_dataset, Alexcriterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgsy0BI211NG"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "torch.save(Alexmodel, \"/content/drive/MyDrive/Beauty/beauty_model\"+current_time+\".pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYQg0pHyHZfB"
      },
      "source": [
        "#Testing part.\n",
        "IMG_ADDRESS = \"/content/Screenshot from 2021-02-03 22-15-52.png\"\n",
        "face_cascade = cv2.CascadeClassifier(\"/content/haarcascade_frontalface_default.xml\")\n",
        "eye_cascade = cv2.CascadeClassifier(\"/content/haarcascade_eye.xml\")\n",
        "img = cv2.imread(IMG_ADDRESS)\n",
        "img = cv2.resize(img, (480, 480), interpolation = cv2.INTER_AREA)\n",
        "img = rotate(img)\n",
        "img = crop(img)\n",
        "img = cv2.resize(img, (227, 227), interpolation = cv2.INTER_AREA)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqO-Rd-HtFxH"
      },
      "source": [
        "img = img.reshape(1,1,227,227)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt7SwedCtafa"
      },
      "source": [
        "img = torch.tensor(img, device = device, dtype= torch.float)\n",
        "outputs = Alexmodel(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYOFmXPBtkIN"
      },
      "source": [
        "torch.argmax(outputs, dim =1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-V6i3zHt6yX"
      },
      "source": [
        "plt.imshow(np.array(img.cpu())[0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS_O8OxYur_O"
      },
      "source": [
        "def rotate(image):\n",
        "  l1=[]\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  if(len(faces)==1):\n",
        "    for (x,y,w,h) in faces:\n",
        "      face_detected = gray[y:y+h,x:x+h]\n",
        "      eyes = eye_cascade.detectMultiScale(face_detected)\n",
        "      for (ex,ey,ew,eh) in eyes:\n",
        "          l1.append((ex+ew/2,ey+eh/2))\n",
        "      \n",
        "      if(len(l1)==2):\n",
        "        dist_x = l1[1][0]-l1[0][0]\n",
        "        dist_y = l1[1][1] - l1[0][1]\n",
        "        if dist_x<0:\n",
        "          dist_y = -dist_y\n",
        "        dist_x = np.abs(dist_x)     \n",
        "        angle = np.arctan(dist_y/dist_x) * 180/3.14\n",
        "        M = cv2.getRotationMatrix2D((240, 240), angle, 1.0)\n",
        "        image = cv2.warpAffine(image, M,(480,480))\n",
        "  return image\n",
        "\n",
        "def crop(image,x_factor=2.5,y_factor=3.5):\n",
        "  l1=[]\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  if(len(faces)==1):\n",
        "    for (x,y,w,h) in faces:\n",
        "      gray = gray[y:y+h,x:x+h]\n",
        "      image = image[y:y+h, x:x+h]\n",
        "  eyes = eye_cascade.detectMultiScale(gray)\n",
        "  for (ex,ey,ew,eh) in eyes:\n",
        "      l1.append((ex+ew/2,ey+eh/2))\n",
        "  if(len(l1)==2):\n",
        "    dist = np.sqrt((l1[0][0]-l1[1][0])**2+(l1[0][1]-l1[1][1])**2)\n",
        "    center_x = image.shape[1]//2\n",
        "    center_y = image.shape[0]//2\n",
        "    shift_x = int(dist*x_factor)//2\n",
        "    shift_y = int(dist*y_factor)//2\n",
        "    start_x = center_x - shift_x\n",
        "    start_x = max(start_x,0)\n",
        "    end_x = center_x+shift_x\n",
        "    end_x = min(end_x,image.shape[1])\n",
        "    start_y = center_y - shift_y\n",
        "    start_y = max(start_y,0)\n",
        "    end_y = center_y + shift_y\n",
        "    end_y = min(end_y,image.shape[0])\n",
        "    image = image[start_y:end_y,start_x:end_x]\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juFXvduqiyNV"
      },
      "source": [
        "IMG_ADDRESS = \"/content/Screenshot from 2021-02-03 22-15-52.png\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87URff-qi05d"
      },
      "source": [
        "img = cv2.imread(IMG_ADDRESS)\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW6WQ4Lei7Hz"
      },
      "source": [
        "def rotate(image):\n",
        "  l1=[]\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  if(len(faces)==1):\n",
        "    for (x,y,w,h) in faces:\n",
        "      face_detected = gray[y:y+h,x:x+h]\n",
        "      eyes = eye_cascade.detectMultiScale(face_detected)\n",
        "      print(eyes)\n",
        "      for (ex,ey,ew,eh) in eyes:\n",
        "          l1.append((ex+ew/2,ey+eh/2))\n",
        "      \n",
        "      if(len(l1)==2):\n",
        "        dist_x = l1[1][0]-l1[0][0]\n",
        "        dist_y = l1[1][1] - l1[0][1]\n",
        "        if dist_x<0:\n",
        "          dist_y = -dist_y\n",
        "        dist_x = np.abs(dist_x)     \n",
        "        angle = np.arctan(dist_y/dist_x) * 180/3.14\n",
        "        M = cv2.getRotationMatrix2D((240, 240), angle, 1.0)\n",
        "        image = cv2.warpAffine(image, M,(480,480))\n",
        "  return image\n",
        "image = rotate(img)\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYEsx5ZcjLTW"
      },
      "source": [
        "def crop(image,x_factor=2,y_factor=3):\n",
        "  l1=[]\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  if(len(faces)==1):\n",
        "    for (x,y,w,h) in faces:\n",
        "      gray = gray[y:y+h,x:x+h]\n",
        "      image = image[y:y+h, x:x+h]\n",
        "  eyes = eye_cascade.detectMultiScale(gray)\n",
        "  for (ex,ey,ew,eh) in eyes:\n",
        "      l1.append((ex+ew/2,ey+eh/2))\n",
        "  if(len(l1)==2):\n",
        "    dist = np.sqrt((l1[0][0]-l1[1][0])**2+(l1[0][1]-l1[1][1])**2)\n",
        "    center_x = image.shape[1]//2\n",
        "    center_y = image.shape[0]//2\n",
        "    shift_x = int(dist*x_factor)//2\n",
        "    shift_y = int(dist*y_factor)//2\n",
        "    start_x = center_x - shift_x\n",
        "    start_x = max(start_x,0)\n",
        "    end_x = center_x+shift_x\n",
        "    end_x = min(end_x,image.shape[1])\n",
        "    start_y = center_y - shift_y\n",
        "    start_y = max(start_y,0)\n",
        "    end_y = center_y + shift_y\n",
        "    end_y = min(end_y,image.shape[0])\n",
        "    image = image[start_y:end_y,start_x:end_x]\n",
        "  return image\n",
        "img_crop = crop(image)\n",
        "plt.imshow(img_crop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJGisYbM-iK6"
      },
      "source": [
        "# Hyperparameter optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL5ZgSSJkFre"
      },
      "source": [
        "def train_optim(config, checkpoint_dir=None):\n",
        "  net = Model_FER(config['l1'], config['l2'])\n",
        "  net.to(device)\n",
        "  criterion = nn.NLLLoss()\n",
        "  optimizer = optim.Adadelta(net.parameters(), lr=config['lr'], weight_decay=1e-5)\n",
        "  if checkpoint_dir:\n",
        "    model_state, optimizer_state = torch.load(\n",
        "        os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "    net.load_state_dict(model_state)\n",
        "    optimizer.load_state_dict(optimizer_state)\n",
        "  train_dataset, val_dataset = get_dataset(greyed_images, labels, (227, 227))\n",
        "  trainloader = DataLoader(train_dataset,batch_size = int(config['batch_size']),shuffle = True, num_workers=8)\n",
        "  valloader = DataLoader(val_dataset,batch_size = int(config['batch_size']),shuffle = True, num_workers=8)\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    epoch_steps = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      epoch_steps += 1\n",
        "      if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "          print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                          running_loss / epoch_steps))\n",
        "          running_loss = 0.0\n",
        "\n",
        "    # Validation loss\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, data in enumerate(valloader, 0):\n",
        "      with torch.no_grad():\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.cpu().numpy()\n",
        "        val_steps += 1\n",
        "\n",
        "    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "      path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "      torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "    tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "  print(\"Finished Training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSW6UlcBC3ey"
      },
      "source": [
        "def test_accuracy(net, device=\"cpu\"):\n",
        "  train_dataset, val_dataset = get_dataset(greyed_images, labels, (227, 227))\n",
        "  valloader = DataLoader(val_dataset,batch_size = 32,shuffle = True, num_workers=8)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "      images, labels = data\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = net(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      outputs = outputs.detach()\n",
        "      outputs = torch.argmax(outputs, dim = 1)\n",
        "      result = torch.sum(outputs==label)\n",
        "      correct+=result.item()\n",
        "  return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfEBC-jDDtPF"
      },
      "source": [
        "def main_optimise(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([8, 16, 32, 64])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_optim),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Rex8ZnE83E"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main_optimise()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddZ1eyJBFDB4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}